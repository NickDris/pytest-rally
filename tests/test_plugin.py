# Licensed to Elasticsearch B.V. under one or more contributor
# license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright
# ownership. Elasticsearch B.V. licenses this file to you under
# the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# 	http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import pytest
import re

# metafunc.parametrize returns this string when given an empty list. 
# Ref1: https://github.com/elastic/pytest-rally/blob/60042c441fc0ca2d6aafe0e298fd7f08e3c30334/pytest_rally/plugin.py#L134
# Ref2: https://docs.pytest.org/en/7.1.x/reference/reference.html#pytest.Metafunc.parametrize
DEFAULT_TRACK_AND_CHALLENGE="track0-challenge0-rally_options0"

class TestPlugin:
    # this should be sorted as per Rally list tracks output
    tracks = [ "many-tracks/sub-track", "many-tracks/sub-track2", "many-tracks/sub-track3", "test-track", "test-track2"]
    challenges = ["index-and-query", "index-only"]

    def test_generates_tests_from_list_tracks(self, pytester, example, temp_repo):
        expected = [
            f"test_track_challenge[{track}-{challenge}]" for track in self.tracks for challenge in self.challenges
        ]
        generated, _ = pytester.inline_genitems(example["all_tracks_and_challenges"], f"--track-repository={temp_repo}")
        assert [func.name for func in generated] == expected

    def test_runs_correct_race_commands(self, caplog, temp_repo, run, example):
        def expected_log_line(track, challenge):
            command = (
                f'esrally race --track="{track}" --challenge="{challenge}" '
                f'--track-repository="{temp_repo}" --track-revision="main" '
                '--configuration-name="pytest" --enable-assertions --kill-running-processes '
                '--on-error="abort" --pipeline="benchmark-only" --target-hosts="127.0.0.1:19200" --test-mode'
            )

            return ("pytest_rally.rally", "INFO", f'Running command: [{command}]')

        challenges = [
            "index-and-query",
            "index-only",
        ]

        expected = [expected_log_line(track, challenge) for track in self.tracks for challenge in challenges]
        res = run(example["all_tracks_and_challenges"])
        actual = [(r.name, r.levelname, r.message) for r in caplog.records if "esrally race" in r.message]
        assert actual == expected

    def test_runs_correct_install_command(self, caplog, temp_repo, distribution_version, revision, source_build_release, run, example):
        install_option = f'--distribution-version={distribution_version}' if distribution_version else f'--revision={revision}'
        if '--revision' in install_option and source_build_release:
            install_option += ' --source-build-release'
        expected = [
            ("pytest_rally.elasticsearch", "DEBUG", 'Installing Elasticsearch: '
            '[esrally install --quiet --http-port=19200 --node=rally-node --master-nodes=rally-node '
            f'--car=4gheap,trial-license,x-pack-ml,lean-watermarks --seed-hosts="127.0.0.1:19300" '
            f'{install_option}]')
        ]
        res = run(example["all_tracks_and_challenges"], install_option)
        commands = [r for r in caplog.records]
        print("\n".join([f"{c.levelname}: {c.message}" for c in commands]))
        actual = [(r.name, r.levelname, r.message) for r in caplog.records if "esrally install" in r.message]
        assert actual == expected

    def test_track_filter_limits_autogenerated_tracks(self, pytester, example, temp_repo):
        all_expected = [
            f"test_track_challenge[{track}-{challenge}]"
            for track in self.tracks
            for challenge in self.challenges
        ]
        expected_map = {
            "": all_expected,
            "test-track2": [
                f"test_track_challenge[test-track2-{challenge}]" for challenge in self.challenges
            ],
            "test-track,test-track2": [
                *[f"test_track_challenge[test-track-{challenge}]" for challenge in self.challenges],
                *[f"test_track_challenge[test-track2-{challenge}]" for challenge in self.challenges],
            ],
            "test-track500": [f"test_track_challenge[{DEFAULT_TRACK_AND_CHALLENGE}]"] ,
            "many-tracks": [
                *[f"test_track_challenge[many-tracks/sub-track-{challenge}]" for challenge in self.challenges],
                *[f"test_track_challenge[many-tracks/sub-track2-{challenge}]" for challenge in self.challenges],
                *[f"test_track_challenge[many-tracks/sub-track3-{challenge}]" for challenge in self.challenges],
            ],
            "many-tracks/sub-track": [
                f"test_track_challenge[many-tracks/sub-track-{challenge}]" for challenge in self.challenges
            ],
        }

        for track_filter, expected in expected_map.items():
            generated, _ = pytester.inline_genitems(
                example["all_tracks_and_challenges"],
                f"--track-repository={temp_repo}",
                f"--track-filter={track_filter}"
            )
            assert [func.name for func in generated] == expected

    def test_track_filter_skips_tracks(self, caplog, temp_repo, example, run):
        expected_map = {
            "": { *self.tracks },
            "test-track2": { "test-track2" },
            "test-track2,test-track": { "test-track", "test-track2" },
            "test-track500": set(),
            "many-tracks/sub-track": { "many-tracks/sub-track", "many-tracks/sub-track2" },
            "many-tracks/sub-track3": { "many-tracks/sub-track3" },
            "many-tracks": { "many-tracks/sub-track", "many-tracks/sub-track2", "many-tracks/sub-track3" },
        }

        for track_filter, expected_tracks in expected_map.items():
            caplog.clear()
            run(example["marked_tracks"], f"--track-filter={track_filter}")
            races = [r for r in caplog.records if "esrally race" in r.message]
            raced_tracks = set()
            for r in races:
                track_match = re.search(r'--track="([^\"]+)"', r.message)
                if track_match:
                    raced_tracks.add(track_match.group(1))
            print(f"track_filter='{track_filter}' => raced_tracks={raced_tracks}, expected_tracks={expected_tracks}")
            assert raced_tracks == expected_tracks

    def test_skip_autogenerated_tests_option(self, pytester, example, temp_repo):
        expected_all = [
            f"test_track_challenge[{track}-{challenge}]"
            for track in self.tracks
            for challenge in self.challenges
        ]
        # Without the flag all autogenerated tests should run
        generated, _ = pytester.inline_genitems(example["all_tracks_and_challenges"], f"--track-repository={temp_repo}")
        assert [func.name for func in generated] == expected_all

        # With flag generated items should be marked as skipped
        generated, _ = pytester.inline_genitems(
            example["all_tracks_and_challenges"],
            f"--track-repository={temp_repo}",
            "--skip-autogenerated-tests"
        )
        assert all("skip" in [m.name for m in func.iter_markers()] for func in generated)
