# Licensed to Elasticsearch B.V. under one or more contributor
# license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright
# ownership. Elasticsearch B.V. licenses this file to you under
# the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# 	http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import pytest
import re

# metafunc.parametrize returns this string when given an empty list. 
# Ref1: https://github.com/elastic/pytest-rally/blob/60042c441fc0ca2d6aafe0e298fd7f08e3c30334/pytest_rally/plugin.py#L134
# Ref2: https://docs.pytest.org/en/7.1.x/reference/reference.html#pytest.Metafunc.parametrize
DEFAULT_TRACK_AND_CHALLENGE="track0-challenge0-rally_options0"

class TestPlugin:
    # this should be sorted as per Rally list tracks output
    tracks = [ "many-tracks/sub-track", "many-tracks/sub-track2", "test-track", "test-track2", "test-track3"]
    challenges = ["index-and-query", "index-only"]

    def test_generates_tests_from_list_tracks(self, pytester, example, temp_repo):
        expected = [
            f"test_track_challenge[{track}-{challenge}]" for track in self.tracks for challenge in self.challenges
        ]
        generated, _ = pytester.inline_genitems(example["all_tracks_and_challenges"], f"--track-repository={temp_repo}")
        assert [func.name for func in generated] == expected

    def test_runs_correct_race_commands(self, caplog, temp_repo, run, example):
        def expected_log_line(track, challenge):
            command = (
                f'esrally race --track="{track}" --challenge="{challenge}" '
                f'--track-repository="{temp_repo}" --track-revision="main" '
                '--configuration-name="pytest" --enable-assertions --kill-running-processes '
                '--on-error="abort" --pipeline="benchmark-only" --target-hosts="127.0.0.1:19200" --test-mode'
            )

            return ("pytest_rally.rally", "INFO", f'Running command: [{command}]')

        challenges = [
            "index-and-query",
            "index-only",
        ]

        expected = [expected_log_line(track, challenge) for track in self.tracks for challenge in challenges]
        res = run(example["all_tracks_and_challenges"])
        actual = [(r.name, r.levelname, r.message) for r in caplog.records if "esrally race" in r.message]
        assert actual == expected

    def test_runs_correct_install_command(self, caplog, temp_repo, distribution_version, revision, run, example):
        install_option = f'--distribution-version={distribution_version}' if distribution_version else f'--revision={revision}'
        expected = [
            ("pytest_rally.elasticsearch", "DEBUG", 'Installing Elasticsearch: '
            '[esrally install --quiet --http-port=19200 --node=rally-node --master-nodes=rally-node '
            f'--car=4gheap,trial-license,x-pack-ml,lean-watermarks --seed-hosts="127.0.0.1:19300" '
            f'{install_option}]')
        ]
        res = run(example["all_tracks_and_challenges"], install_option)
        actual = [(r.name, r.levelname, r.message) for r in caplog.records if "esrally install" in r.message]
        assert actual == expected

    def test_track_filter_limits_autogenerated_tracks(self, pytester, example, temp_repo):
        def expected_test_names(track_filter):
            filter_items = None if track_filter == "" else [t.strip() for t in track_filter.split(",")]
            if filter_items:
                result = []
                for track in self.tracks:
                    if any(track == tf or track.split('/')[0] == tf for tf in filter_items):
                        result += [f"test_track_challenge[{track}-{challenge}]" for challenge in self.challenges]
                if not result:
                    result = [f"test_track_challenge[{DEFAULT_TRACK_AND_CHALLENGE}]"]
            else:
                result = [
                    f"test_track_challenge[{track}-{challenge}]"
                    for track in self.tracks
                    for challenge in self.challenges
                ]
            return result

        test_track_filters = ["","test-track2", "test-track,test-track2", "test-track500", "many-tracks", "many-tracks/sub-track"]
        for track_filter in test_track_filters:
            expected = expected_test_names(track_filter)
            generated, _ = pytester.inline_genitems(
                example["all_tracks_and_challenges"],
                f"--track-repository={temp_repo}",
                f"--track-filter={track_filter}"
            )
            assert [func.name for func in generated] == expected

    def test_track_filter_skips_tracks(self, caplog, temp_repo, example, run):
        # Current implementation cannot support testing a list of markers in the test functions
        # because it needs to find expected tracks based on the race commands for each function
        def expected_tracks_filtered(track_filter):
            result = set()
            filter_items = None if track_filter == "" else [t.strip() for t in track_filter.split(",")]
            if filter_items:
                for track in self.tracks:
                    if any(track == tf or track.split('/')[0] == tf for tf in filter_items):
                        result.add(track)
            else:
                result = set([track for track in self.tracks])
            return result

        test_track_filters = ["", "test-track2", "test-track2,test-track", "test-track500", "many-tracks", "many-tracks/sub-track"]
        for track_filter in test_track_filters:
            caplog.clear()
            run_function = run(example["marked_tracks"], f"--track-filter={track_filter}")
            races = [r for r in caplog.records if "esrally race" in r.message]
            raced_tracks = []
            for r in races:
                track_match = re.search(r'--track="([^"]+)"', r.message)
                if track_match:
                    raced_tracks.append(track_match.group(1))
            expected_tracks = expected_tracks_filtered(track_filter)
            actual_tracks = set(raced_tracks)
            assert actual_tracks == expected_tracks

    def test_skip_autogenerated_tests_option(self, pytester, example, temp_repo):
        expected_all = [
            f"test_track_challenge[{track}-{challenge}]"
            for track in self.tracks
            for challenge in self.challenges
        ]
        # Without the flag all autogenerated tests should run
        generated, _ = pytester.inline_genitems(example["all_tracks_and_challenges"], f"--track-repository={temp_repo}")
        assert [func.name for func in generated] == expected_all

        # With flag generated items should be marked as skipped
        generated, _ = pytester.inline_genitems(
            example["all_tracks_and_challenges"],
            f"--track-repository={temp_repo}",
            "--skip-autogenerated-tests"
        )
        assert all("skip" in [m.name for m in func.iter_markers()] for func in generated)
