# Licensed to Elasticsearch B.V. under one or more contributor
# license agreements. See the NOTICE file distributed with
# this work for additional information regarding copyright
# ownership. Elasticsearch B.V. licenses this file to you under
# the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# 	http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

import pytest
import re

# metafunc.parametrize returns this string when given an empty list. 
# Ref1: https://github.com/elastic/pytest-rally/blob/60042c441fc0ca2d6aafe0e298fd7f08e3c30334/pytest_rally/plugin.py#L134
# Ref2: https://docs.pytest.org/en/7.1.x/reference/reference.html#pytest.Metafunc.parametrize
DEFAULT_TRACK_AND_CHALLENGE="track0-challenge0-rally_options0"

class TestPlugin:
    # this should be sorted as per Rally list tracks output
    tracks = ["test-track", "test-track2", "test-track3"]
    challenges = ["index-and-query", "index-only"]

    def test_generates_tests_from_list_tracks(self, pytester, example, temp_repo):
        expected = [
            f"test_track_challenge[{track}-{challenge}]" for track in self.tracks for challenge in self.challenges
        ]
        generated, _ = pytester.inline_genitems(example["all_tracks_and_challenges"], f"--track-repository={temp_repo}")
        assert [func.name for func in generated] == expected

    def test_runs_correct_race_commands(self, caplog, temp_repo, run):
        def expected_log_line(track, challenge):
            command = (
                f'esrally race --track="{track}" --challenge="{challenge}" '
                f'--track-repository="{temp_repo}" --track-revision="main" '
                '--configuration-name="pytest" --enable-assertions --kill-running-processes '
                '--on-error="abort" --pipeline="benchmark-only" --target-hosts="127.0.0.1:19200" --test-mode'
            )

            return ("pytest_rally.rally", "INFO", f'Running command: [{command}]')

        challenges = [
            "index-and-query",
            "index-only",
        ]

        expected = [expected_log_line(track, challenge) for track in self.tracks for challenge in challenges]
        res = run()
        actual = [(r.name, r.levelname, r.message) for r in caplog.records if "esrally race" in r.message]
        assert actual == expected

    def test_runs_correct_install_command(self, caplog, temp_repo, run):
        expected = [
            ("pytest_rally.elasticsearch", "DEBUG", 'Installing Elasticsearch: '
            '[esrally install --quiet --http-port=19200 --node=rally-node --master-nodes=rally-node '
            '--car=4gheap,trial-license,x-pack-ml,lean-watermarks --seed-hosts="127.0.0.1:19300" '
            '--revision=current]')
        ]
        res = run()
        actual = [(r.name, r.levelname, r.message) for r in caplog.records if "esrally install" in r.message]
        assert actual == expected

    def test_track_filter_limits_autogenerated_tracks(self, pytester, example, temp_repo):
        def expected_test_names(track_filter):
            filter_items = None if track_filter == "" else [t.strip() for t in track_filter.split(",")]
            if filter_items and all(f not in self.tracks for f in filter_items):
                result = [f"test_track_challenge[{DEFAULT_TRACK_AND_CHALLENGE}]"]
            else:
                result = [
                    f"test_track_challenge[{track}-{challenge}]"
                    for track in self.tracks if not filter_items or track in filter_items
                    for challenge in self.challenges
                ]
            return result
        
        test_track_filters = ["","test-track2", "test-track,test-track2", "test-track500"]
        for track_filter in test_track_filters:
            expected = expected_test_names(track_filter)
            generated, _ = pytester.inline_genitems(
                example["all_tracks_and_challenges"],
                f"--track-repository={temp_repo}",
                f"--track-filter={track_filter}"
            )
            assert [func.name for func in generated] == expected

    def test_track_filter_skips_tracks(self, caplog, temp_repo, example, run_with_filter):
        def expected_tracks_filtered(track_filter):
            filter_items = None if track_filter == "" else [t.strip() for t in track_filter.split(",")]
            return set([track for track in self.tracks if not filter_items or track in filter_items])
        
        test_track_filters = ["", "test-track2", "test-track2,test-track", "test-track500"]
        for track_filter in test_track_filters:
            caplog.clear()
            run_function = run_with_filter(track_filter, example["marked_tracks"])
            races = [r for r in caplog.records if "esrally race" in r.message]
            raced_tracks_challenges_dict = {}
            for r in races:
                track_match = re.search(r'--track="([^"]+)"', r.message)
                if track_match:
                    raced_tracks_challenges_dict.setdefault("tracks", []).append(track_match.group(1))
            expected_tracks = expected_tracks_filtered(track_filter)
            actual_tracks = set(raced_tracks_challenges_dict.get("tracks", []))
            assert actual_tracks == expected_tracks

    def test_skip_autogenerated_tests_option(self, pytester, example, temp_repo):
        expected_all = [
            f"test_track_challenge[{track}-{challenge}]"
            for track in self.tracks
            for challenge in self.challenges
        ]
        # Without the flag all autogenerated tests should run
        generated, _ = pytester.inline_genitems(example["all_tracks_and_challenges"], f"--track-repository={temp_repo}")
        assert [func.name for func in generated] == expected_all

        # With flag generated items should be marked as skipped
        generated, _ = pytester.inline_genitems(
            example["all_tracks_and_challenges"],
            f"--track-repository={temp_repo}",
            "--skip-autogenerated-tests"
        )
        assert all("skip" in [m.name for m in func.iter_markers()] for func in generated)